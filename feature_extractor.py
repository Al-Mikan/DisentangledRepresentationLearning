# feature_extractor.py

import os
import csv
import json
import numpy as np
import torch
import decord
import pandas as pd
from collections import Counter
from transformers import VideoMAEImageProcessor, VideoMAEModel

# ----------------------
# â‘  CSVèª­ã¿è¾¼ã¿ã¨mammalç¨®ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ100ä»¶ä»¥ä¸Šï¼‰
# ----------------------
df = pd.read_csv("labels.csv")
df["species"] = df["species"].str.strip()
df["parent_class"] = df["parent_class"].str.strip().str.lower()
df["action"] = df["action"].str.strip()

mammals = df[df["parent_class"] == "mammal"]
species_counts = Counter(mammals["species"])

action_counts = Counter(mammals["action"])
valid_actions = {a for a, c in action_counts.items() if c >= 100}

print(f"âœ… 100ä»¶ä»¥ä¸Šã‚ã‚‹è¡Œå‹•: {len(valid_actions)} ç¨®é¡")
print("ğŸ¯ å¯¾è±¡è¡Œå‹•ä¸€è¦§:")
for a in sorted(valid_actions):
    print(f"  - {a}")

filtered_df = mammals[mammals["action"].isin(valid_actions)]

filtered_df.to_csv("filtered_labels.csv", index=False)
print(f"âœ… filtered_labels.csv ã‚’ {len(filtered_df)} ä»¶ã§ä¿å­˜ã—ã¾ã—ãŸã€‚")

# ----------------------
# â‘¡ åˆæœŸè¨­å®šã¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
# ----------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
decord.bridge.set_bridge("torch")

processor = VideoMAEImageProcessor.from_pretrained("MCG-NJU/videomae-base")
model = VideoMAEModel.from_pretrained("MCG-NJU/videomae-base").to(device).eval()

# ----------------------
# â‘¢ å‹•ç”» â†’ ãƒ™ã‚¯ãƒˆãƒ«å¤‰æ›é–¢æ•°
# ----------------------
def video_to_vec(path, n_frames=16):
    try:
        vr = decord.VideoReader(path)
        total_frames = len(vr)
        if total_frames < n_frames:
            print(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒå°‘ãªã„: {path}ï¼ˆ{total_frames}ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰")
            return None

        idx = np.linspace(0, total_frames - 1, n_frames).astype(np.int64)
        frames = vr.get_batch(idx).permute(0, 3, 1, 2).float() / 255.0  # (n_frames, C, H, W)

        inputs = processor(list(frames), return_tensors="pt", do_rescale=False).to(device)

        with torch.no_grad():
            outputs = model(**inputs)
        z = outputs.last_hidden_state[:, 0]  # [CLS] token
        return z.squeeze(0).cpu().numpy().tolist()
    except Exception as e:
        print(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {path} â†’ {e}")
        return None

# ----------------------
# â‘£ ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºã¨ä¿å­˜
# ----------------------
vectors = {}
total_count = 0
skipped_count = 0

with open('filtered_labels.csv', newline='', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for row in reader:
        total_count += 1
        rel_path = row['video_path'].replace('\\', '/')
        full_path = os.path.join('./', rel_path)

        vec = video_to_vec(full_path)
        if vec is not None:
            vectors[rel_path] = vec
        else:
            skipped_count += 1

with open('video_vectors.json', 'w', encoding='utf-8') as f:
    json.dump(vectors, f)

# ----------------------
# â‘¤ çµæœã®ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
# ----------------------
print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°ãƒ¬ãƒãƒ¼ãƒˆ")
print(f"ğŸ—‚ï¸ filtered_labels.csv ã®ä»¶æ•°        : {total_count}")
print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«æŠ½å‡ºã«æˆåŠŸã—ãŸä»¶æ•°       : {len(vectors)}")
print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸä»¶æ•°ï¼ˆå¤±æ•—/çŸ­ã™ãï¼‰: {skipped_count}")

